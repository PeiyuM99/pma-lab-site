---
name: Agent Evaluation Benchmark
summary: Benchmarks for reliability, latency, and safety in real-world coding-agent workflows.
status: planned
lead: Peiyu Ma
image: /images/projects/eval-bench.svg
publications:
  - Ma P., Reliability Benchmarks for Autonomous Tool Use, 2026 (in preparation)
  - Ma P., Measuring Failure Modes in Multi-Step Agent Tasks, 2025 (preprint)
funding:
  - Proposed support for trustworthy embodied/interactive AI evaluation (2026)
links:
  website: https://example.org/eval-bench
order: 2
---

This project develops reproducible test suites and metrics to evaluate coding-agent behavior under uncertainty.

The benchmark targets practical dimensions including execution reliability, cost/latency trade-offs, and safety compliance.
